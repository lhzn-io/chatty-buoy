# chatty-buoy

Local AI inference serving project using kanoa-mlops infrastructure templates.

## Overview

chatty-buoy is a demonstration project that uses the kanoa-mlops plugin to bootstrap and manage local LLM serving with vLLM and Ollama runtimes.

## Setup

### Prerequisites

- Docker & Docker Compose
- NVIDIA GPU with CUDA support
- NVIDIA Container Toolkit
- Python 3.11+ with conda

### Installation

```bash
# Create conda environment
conda env create -f environment.yml
conda activate chatty-buoy

# Initialize infrastructure from kanoa-mlops templates
kanoa init mlops --dir .
```

⚠️ **Important**: The `docker/` directory is generated by `kanoa init mlops` and is not tracked in git. Run the init command to generate the latest infrastructure configs from kanoa-mlops templates.

## Usage

### Interactive Mode

```bash
# Start any vLLM or Ollama service interactively
kanoa serve

# List available models
kanoa list

# Check service status
kanoa status
```

### Direct Commands

```bash
# Start specific service
kanoa serve vllm molmo
kanoa serve vllm gemma3 --model google/gemma-3-12b-it

# Stop services
kanoa down vllm molmo
```

## Models

Current models configured:

- **gemma3**: Google Gemma 3 (12B, 27B variants)
- **molmo**: AllenAI Molmo 7B (multimodal)
- **olmo3**: AllenAI OLMo 3 (7B, 32B variants)

Models are cached in `~/.cache/huggingface/hub/`.

## Docker Services

Services are defined in `docker/vllm/` and `docker/ollama/`:

- `docker-compose.gemma3.yml` - Gemma 3 inference
- `docker-compose.molmo.yml` - Molmo multimodal
- `docker-compose.olmo3.yml` - OLMo 3 inference
- `docker-compose.ollama.yml` - Ollama runtime

## Architecture

Built on NVIDIA Jetson Thor (Blackwell sm_110) using custom vLLM image:
`ghcr.io/nvidia-ai-iot/vllm:latest-jetson-thor`

## License

MIT
